Neural Networks algorithms

This work started with the aim to understand the internal dynamics of neural networks. So we will try to use as little libraries as possible. The general idea is to develop neural networks in the mathematical language of tensors and escape from the limitations of linear algebra and matrix calculus, in hope of allowing ML algorithms to handle more complex data.

Currently there are four main networks in this work. One is LSTM (Long short term memory), two SOM (self-organizing map), three Convolutional Neural Network (CNN) and a fully connected network (FCN). 

The SOM is somewhat more complete, whereas the rest luck for now the training part. For gradient descend method the idea is to use symbolic differentiation and use the computed gradients to update the weights and biases. We argue that the gradient of a network is only determined from the network's architecture. So we want to 'differentiate' once, during initiation, and from there use the expressions to find which elements should be used to update the parameters. Another reason for this approach is that the actual derivatives (covariant/contravariant derivatives) of tensors include other terms (connection terms). This terms vanish in Euclidean (Flat) space but they do not in curved spaces. In the future we might want to include the possibility that the space the network's parameters (weights, biases) exist is not the Euclidean space and to do so we will need to use differential geometry. We also keep in mind that neural networks can be trained by other methods like competitive learning (as we do in SOM) and associative learning. In SOM the basic building blocks of the Convolutional Neural Network (CNN) can be found.

In the future all the building blocks of each network will be class methods of only one master network. So for example the gates from LSTM will be methods in FCN. The architecture of the network will be passed as a dictionary during initiation and all the connections will be created. The structure of all algorithms is such that the architecture and every parameter can be passed as a python dictionary with strings and numbers. This is so that the networks can be used under an evolutionary algorithm and even try and combine with other ML algorithms. Every parameter of the network, from hyper-parameters (number of neurons, learning rate etc.) to which activation functions are used (even the activation functions are allowed to have parameters) can be used as the genes of an evolutionary algorithm. But for now we keep them in separated branches to test them individually.

Also an Elman Recurrent network is included, purely for demonstration purposes, as to how RNNs flow and how real time on-line training might be conducted.

For the mathematics of neural networks we rely on the book *'Neural Network Design (2nd Edition)'* by Martin T Hagan, Howard B Demuth, Mark H Beale, Orlando De Jes√∫s.
